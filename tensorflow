# check if gpu is detected
tf.test.is_gpu_available()

# reduce logging to show only errors
import logging
tf.get_logger().setLevel(logging.ERROR)

# sometimes loading images produces errors when predicting
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True

# check what this does
test_generator.reset()

# For tensorflow1, to find out which device is used, you can enable log device placement like this:
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))

# plot model
tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)

# fix issue with tensorflow not showing INFO when run from cmd line but shows in notebook
# DEPRECATED!
import logging
tf.logging.set_verbosity(tf.logging.INFO)

# supported version
import logging
tf.get_logger().setLevel(logging.WARNING)

# in order to view the content of a dataset object in tf 1.x you need to enable eager execution before any other
# tf operations are done
import tensorflow as tf
tf.enable_eager_execution()

#
import tensorflow as tf
import tensorflow_datasets as tfds # requires pip install tensorflow-datasets=3.2.1 for compatibility with py3.7

ds = tfds.load('iris', split='train', shuffle_files=True)

# check shape from generator
for image_batch, labels_batch in train_ds:
  print(image_batch.shape)
  print(labels_batch.shape)
  print(image_batch[0].numpy().shape)
  break

# Retrieve filenames as single list from tf dataset which is batched
    test_ds = tf.data.Dataset.load(test_dir)
    test_ds = tf.keras.utils.image_dataset_from_directory(
      data_dir,
      #color_mode='grayscale',
      shuffle=False,
      seed=123,
      image_size=(img_height, img_width),
      batch_size=batch_size)
 
    normalization_layer = layers.Rescaling(1./255)

    def change_inputs(images, labels, paths):
      x = normalization_layer(images)
      return x, labels,  tf.constant(paths)

    normalized_ds = test_ds.map(lambda images, labels: change_inputs(images, labels, paths=test_ds.file_paths))
    AUTOTUNE = tf.data.AUTOTUNE
    normalized_ds = normalized_ds.cache().prefetch(buffer_size=AUTOTUNE)


# getting positive and negative sample count
positive_count = dataset.map(lambda _, label: tf.reduce_sum(label)).reduce(0, lambda count, val: count + val)
negative_count = dataset.map(lambda _, label: tf.reduce_sum(1 - label)).reduce(0, lambda count, val: count + val)

print("Positive class count:", positive_count.numpy())
print("Negative class count:", negative_count.numpy())

# see all keys in history
print(history.history.keys())	

# plot a single image from a tensorflow dataset
    image, label = next(iter(train_ds))
    image_data = image.numpy().astype(np.uint8)
    plt.imshow(image_data)
    plt.show()

# test train split for tf.data.Dataset

validation_size = int(0.2 * total_size)
zipped_dataset = zipped_dataset.shuffle(buffer_size=total_size)
train_dataset = zipped_dataset.skip(validation_size)
validation_dataset = zipped_dataset.take(validation_size)

# compute dataset size

# Define a function to accumulate the size of the dataset
def accumulate_size(count, _):
        return count + 1

# Initialize a counter
initial_count = tf.constant(0)

# Use the reduce method to count the elements in the dataset
dataset_size = dataset.reduce(initial_count, accumulate_size)
dataset_size = dataset_size.numpy()


# to only run on CPU wrap the fitting code with following
    with tf.device('/CPU:0'):

# verbosity
0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' becomes 1 for most cases
Note that the progress bar is not particularly useful when logged to a file, 
so verbose=2 is recommended when not running interactively. 

# dataset size
# this returns -1 is filter has been used 
ds.cardinality().numpy()

# unwrap data from dataset
one_item_ds = ds.take(1)
iterator = iter(one_item_ds)
images, labels = next(iterator)

# get random data batch from dataset
iterator = iter(shuffled_ds)
random_batch = iterator.get_next()
images_batch, labels_batch = random_batch# expand dimensions
features = tf.expand_dims(features, axis=1)

# show tensor shape
print(features.shape)

# load weights
model.load_weights("weights_path.h5") # model is the compiled model

# validation split
model.fit(..validation_split=0.2)
model.fit(validation_data=val_ds)

# length of dataset
len(list(dataset))

# get expected model input shape
config = model.get_config() # Returns pretty much every information about your model
print(config["layers"][0]["config"]["batch_input_shape"]) # returns a tuple of width, height and channels
