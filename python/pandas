# basic search using column value conditions
df.loc[df['Class']=="Barred"]

# sort using column
df.sort_values(colname,ascending=True) # or df.sort_values(by=colname)
# if you want to reset index after sort use ignore_index=True

# search if column value is in a particular range
pd.Series.between(0,1)

# clean data
# count number of missing elements in each column
counter_nan  = df_all.isnull().sum()

counter_without_nan = counter_nan[counter_nan==0]

# remove columns with missing elements
df_all = df_all[counter_without_nan.keys()]

# Here filtering the rows according to
# Grade column which has notnull value.
df = df.loc[df['Grade'].notnull()]

# find 75th percentile for a particular column
df['colname'].quantile(0.75)

# rename columns
df.rename(columns={oldcol:newcol,oldcol2:newcol2})

# convert boolean series to integer
Series.astype(int)

# convert pandas series to list
Series.values.tolist()

# reset index
df.reset_index(drop=True)

# create dataframe from multidimensional lists
# List1
lst = [['tom', 'reacher', 25], ['krish', 'pete', 30],
       ['nick', 'wilson', 26], ['juli', 'williams', 22]]

df = pd.DataFrame(lst, columns =['FName', 'LName', 'Age'], dtype = float)
df

# display all rows
DataFrame.to_string(index=False)

# create a series from a list
pd.DataFrame(mylist,columns=['id'])

# pandas version of np.argmax
# Return index of first occurrence of maximum over requested axis.
df.idxmax

# shuffle dataframe
# the default random state is derived from the system time
#for reproducability specify a random state
df.sample(frac=1, random_state=42)

# to reset the index after shuffling
df = df.sample(frac=1).reset_index(drop=True)

# read data from csv without index duplicate
pd.read_csv('path',index_col=0)

# index pandas dataframe using a numpy array of indices
df.iloc[indarr] # df.iloc[:50] does same as df[:50]

# select rows whose column values in an array
df.loc[df['favorite_color'].isin(array)]

# select inverse of indexes of a numpy array
mask = np.ones(len(data), np.bool)
mask[sample_indexes] = 0
other_data = data[mask]

# join two dataframes
pd.merge(df1,df2,on='column')

# to merge two dataframes without any common columns
# make the indexes match by ordering both and resetting the indexes
# then merge using left_index=True and right_index=True

# join two dataframes in sql style
pd.merge(df1, df2, left_on="col1", right_on="col1", validate="one_to_one", how="inner")

# split dataframe into n parts
np.array_split(df, 3)

# reading excel data
pip install openpyxl
df = pd.read_excel("Book1.xlsx",sheet_name=0)

# parse dates
df = pd.read_csv('data/data_3.csv', parse_dates=['date'])

# skipping footer the number you pass is the number of lines to skip beginning from bottom
df = pd.read_csv(skipfooter=12..)

# remove columsn
df.drop("colname",axis=1)

# reading columns as float instead of int
# if some values are missing in a numerical column pandas treats the columns as float as NaN is considered a float
# This might lead to data loss check other fields in the dropped rows
df.dropna(inplace=True)
df.a = df.a.astype(int)
df.b = df.b.astype(int)

# read commas in numbers
 pd.read_csv(thousands=',')

# describe all options
pd.describe_option()

# read dates from excel file
df1 = pd.read_excel(file, converters= {'COLUMN': pd.to_datetime})

# convert any date column to date datatype
# check if day or month comes first
pd.to_datetime(Series,dayfirst=True)

# read only some columns
pd.read_csv(usecols=[]) # usecols=range(8) for the first 9 columns

# avoid copy warning by creating a copy of the original and setting on the copy
df_copy = cdf_sorted.copy()
df_copy['radius_rank'] = df_copy['radius'].rank(ascending=False,pct=True)

# remove duplicates on only specific columns
df = df.drop_duplicates(subset=['ID','Order Date'], keep='last')

# combine conditions in row selection
df[(df['psfMajorFWHM']>=2) & (df['kronRad'].between(10,14))] # always use the brackets otherwise you may get unexpected results

import pandas as pd
df = pd.read_excel("export?format=xlsx",sheet_name=0)
df.to_csv("python_dress.csv")

# random sample
df.sample(10)

# convert pandas timestamp to isoformat
Timestamp.isoformat()

# iterate over pandas dataframe rows and access by index or column name
for _, row in df.iterrows():
        print(row['colname']) # or row[0]

# find duplicates
duplicates = df1[df1.duplicated(subset='key')]

# create dataframe from list of dicts
pd.DataFrame(listname)

# save latex code to tex file
# \usepackage{booktabs}
# might have to install the following for longtable support for tables that extend over pages
pip install Jinja2
df.to_latex(buf="path/to/save", longtable=True, index=False)

# iterate over groups in groupby
for group_key, group_df in df.groupby(['col1', 'col2']):

# to get an individual group
grouped = df.groupby()
grouped.get_group((key1,key2))

# query dataframe 
df.query('colname == value')

# rename pandas columns using a list of names
master_df.set_axis(col_names, axis=1)

# best practice
It is not recommended to build DataFrames by adding single rows in a for loop. Build a list of rows and make a DataFrame in a single concat.

# pd.concat
 Any None objects will be dropped silently unless they are all None in which case a ValueError will be raised

# remove the index and place it as a new column
df.reset_index(names=['newindex'])

# using pandas df.apply
df.apply(lambda x: os.path.basename(x['first_filenames']), axis=1)

# drop rows bases on criteria
# Identify the index of rows that meet the criteria
index_to_drop = df[criteria].index

# Drop the rows
df = df.drop(index_to_drop)

# (Optional) Reset the index
df = df.reset_index(drop=True)

# replace placeholder data with nan to get proper statistics
df.replace(-999999, np.nan, inplace=True)

# round to two decimal places
pd.options.display.float_format = '{:.2f}'.format

# select string datatype
Pass ``string`` or pd.StringDtype() to dtype parameter
